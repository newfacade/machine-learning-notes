{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "ensemble learning: combine multiple weak leaners to form a strong learner.\n",
    "\n",
    "bagging is one approach of ensemble learning.\n",
    "\n",
    "it uses some randomness, e.g sampling at random, then generate a bunch of different base learner. \n",
    "\n",
    "when we say sampling at random, we mean sampling with replacement. \n",
    "\n",
    "we usually select $m$ samples with replacement from m samples.\n",
    "\n",
    "when predicting:\n",
    "\n",
    "1. if classification, use vote.\n",
    "2. if regression, use mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"make moons dataset\"\"\"\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)  # default test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create ensemble learning\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting=\"hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "\"\"\"training and testing\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9253333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"set oob_score=True, so each sample can be evalute on estimators whose bag does not include that sample\"\"\"\n",
    "bag_oob_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            max_samples=100, bootstrap=True, n_jobs=-1, random_state=42, oob_score=True)\n",
    "bag_oob_clf.fit(X_train, y_train)\n",
    "bag_oob_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_oob_pred = bag_oob_clf.predict(X_test)\n",
    "np.all(y_oob_pred == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random foreset\n",
    "\n",
    "random foreset is one implementation of bagging with decision tree as base learner.\n",
    "\n",
    "besides sampling at random, random foreset added some random when split.\n",
    "\n",
    "rather than using all $d$ features when split, we randomly select it's $k$ features subset, and select from this subset.\n",
    "\n",
    "often, we set $k = log_{2}d$.\n",
    "\n",
    "in one word:\n",
    "\n",
    "random foreset $=$ decision tree $+$ bagging $+$ use random subset when split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"roughly equivalent\"\"\"\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16), n_estimators=500,\n",
    "                            max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09127620679978103\n",
      "sepal width (cm) 0.02188742948925206\n",
      "petal length (cm) 0.43876122766425646\n",
      "petal width (cm) 0.4480751360467105\n"
     ]
    }
   ],
   "source": [
    "\"\"\"RandomForest automaticly measures feature importance = weighted average of node's impurity reduce\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting\n",
    "\n",
    "boosting is another approach of ensemble learning.\n",
    "\n",
    "boosting first train a base learner which is rather weak, then adjust that leaner with the knowledge of current prediction to make it stronger.\n",
    "\n",
    "for example, adjust the traing set distribution, focus on samples that previous predict wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adaboost\n",
    "\n",
    "suppose a binary classification problem, dataset is $\\left\\{(x_{1}, y_{1}),...,({x_{N},y_{N}})\\right\\}$, $y_{i} \\in \\left\\{-1, 1\\right\\}$, total $N$ samples.\n",
    "\n",
    "initial sample distribution is $D_{1} = (\\frac{1}{N},...,\\frac{1}{N}) = (w_{1,1},...,w_{1,N})$\n",
    "\n",
    "for $m=1,...,M$:\n",
    "\n",
    "1. train $G_{m}: \\mathcal{X} \\rightarrow \\left\\{-1, 1\\right\\}$ based on $D_{m} = (w_{m,1},...,w_{m,N})$\n",
    "\n",
    "2. compute misclassify error:\n",
    "$$e_{m} = \\sum_{i=1}^{N}P(G_{m}(x_{i}) \\ne y_{i}) = \\sum_{i=1}^{N}w_{m,i}I(G_{m}(x_{i} \\ne y_{i}))$$\n",
    "3. update distribution $D_{m}$ to $D_{m+1}$:\n",
    "$$\n",
    "w_{m+1, i} =\n",
    "\\begin{cases}\n",
    "\\frac{w_{m,i}}{Z_{m}} \\\\\n",
    "{\\frac{1 - e_{m}}{e_{m}}}\\frac{w_{m,i}}{Z_{m}}\n",
    "\\end{cases}\n",
    "$$\n",
    "where $Z_{m}$ is the normalization factor that makes $D_{m+1}$ a proper distribution.<br>\n",
    "this step simply enlarge the previous wrong predicted sample's distribution by a factor $\\frac{1 - e_{m}}{e_{m}}$.\n",
    "\n",
    "then construct a linear combination of $G_{m}$:\n",
    "\n",
    "$$f(x) = \\sum_{m=1}^{M}log\\ \\frac{1 - e_{m}}{e_{m}}G_{m}(x)$$\n",
    "\n",
    "finally the resulting classifier:\n",
    "\n",
    "$$G(x) = sign(f(x)) = sign\\left(\\sum_{i=1}^{N}log\\ \\frac{1 - e_{m}}{e_{m}}G_{m}(x)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100, algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward stage wise algorithm\n",
    "\n",
    "consider addictive model:\n",
    "\n",
    "$$f(x) = \\sum_{m=1}^{M}\\beta_{m}b(x; \\gamma_{m})$$\n",
    "\n",
    "here $b$ is the base model, $\\gamma_{m}$ is it's paramter.\n",
    "\n",
    "given loss function $L$, we try to solve the following optimization problem:\n",
    "\n",
    "$$\\underset{\\beta, \\gamma}{min}\\sum_{i=1}^{N}L\\left(y_{i}, \\sum_{m=1}^{M}\\beta_{m}b(x; \\gamma_{m})\\right)$$\n",
    "\n",
    "this is usually very complex.\n",
    "\n",
    "foward stage wise algorithm simplify it by going only one stage at a time:\n",
    "\n",
    "init $f_{0}(x) = 0$.\n",
    "\n",
    "for $m=1,...,M$:\n",
    "\n",
    "1. do the following one step optimization:\n",
    "$$(\\beta_{m}, \\gamma_{m}) = \\underset{\\beta,\\gamma}{argmin}\\sum_{i=1}^{N}L(y_{i}, f_{m-1}(x_{i}) + \\beta{b(x_{i},\\gamma)})$$\n",
    "2. update:\n",
    "$$f_{m}(x) = f_{m-1}(x) + \\beta_{m}b(x;\\gamma_{m})$$\n",
    "\n",
    "finally the resulting addictive model:\n",
    "\n",
    "$$f(x) = f_{M}(x) = \\sum_{m=1}^{M}\\beta_{m}b(x; \\gamma_{m})$$\n",
    "\n",
    "adaboost $\\Leftrightarrow$ foward stage wise algorithm when $L(y, f(x)) = exp(-yf(x))$, t.b.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting tree\n",
    "\n",
    "boosting tree model:\n",
    "\n",
    "$$f(x) = \\sum_{m=1}^{M}T(x; \\theta_{m})$$\n",
    "\n",
    "where $T(x; \\theta_{m})$ is a decision tree.\n",
    "\n",
    "boosting tree use the forward stage wise algorithm.\n",
    "\n",
    "for binary classification boosting tree, we use $L(y, f(x)) = exp(-yf(x))$, so equivalent to adaboost where base model is decision tree.\n",
    "\n",
    "for regression problem, we use $L(y, f(x)) = (y - f(x))^2$, i.e square error.\n",
    "\n",
    "then the forward stage wise algorithm:\n",
    "\n",
    "$$\\hat\\theta_{m} = \\underset{\\theta_{m}}{argmin}\\sum_{i=1}^{N}L(y_{i}; f_{m-1}(x) + T(x_{i};\\theta_{m}))$$\n",
    "\n",
    "$$L(y_{i}; f_{m-1}(x) + T(x_{i};\\theta_{m})) = [y - f_{m-1}(x) - T(x;\\theta_{m})]^{2} = [r - T(x;\\theta_{m})]^{2}$$\n",
    "\n",
    "here $r = y - f_{m-1}(x)$ is the residual.\n",
    "\n",
    "so for regression tree, we just need to fit the residual!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT-gradient boosting decision tree\n",
    "\n",
    "while we can easily proceed in forward stage wise algorithm if loss function is square or exponential, for generic loss, this is not easy.\n",
    "\n",
    "we use the gradient of the loss function to proceed  in generic case, that is GBDT.\n",
    "\n",
    "GBDT for regression:\n",
    "\n",
    "input: training set $T = \\left\\{(x_{1},y_{1}),...,(x_{N},y_{N})\\right\\}$, $x_{i} \\in \\mathcal{X} \\in \\mathbb{R}^{n}$, $y_{i} \\in \\mathcal{Y} \\in \\mathbb{R}$, loss function $L(y, f(x))$.\n",
    "\n",
    "output: regression tree $\\hat{f}(x)$.\n",
    "\n",
    "init \n",
    "$$f_{0}(x) = \\underset{c}{argmin}\\sum_{i=1}^{N}L(y_{i}, c)$$\n",
    "\n",
    "for $m=1,...,M$.\n",
    "\n",
    "1. for $i=1,...,N$, compute\n",
    "$$r_{mi} = -\\left[\\frac{\\partial{L(y_{i}, f(x_{i}))}}{\\partial f(x_{i})}\\right]_{f = f_{m-1}}$$\n",
    "\n",
    "2. generate a tree that fits $r_{mi}$, denote it's leaf nodes areas as $R_{mi},j=1,...,J$.\n",
    "\n",
    "3. for $c=1,...,J$, compute\n",
    "$$c_{mj} = \\underset{c}{argmin}\\sum_{x_{i} \\in R_{mj}}L(y_{i}, f_{m-1}(x_{i}) + c)$$\n",
    "\n",
    "4. update $f_{m}(x) = f_{m-1}(x) + \\sum_{j=1}^{J}c_{mj}I(x \\in R_{mj})$.\n",
    "\n",
    "finally the resulting regression tree:\n",
    "\n",
    "$$\\hat{f}(x) = f_{M}(x) = \\sum_{m=1}^{M}\\sum_{j=1}^{J}c_{mj}I(x \\in R_{mj})$$\n",
    "\n",
    "in one word, while we want to minimize $L(y_{i}, f_{m-1}(x_{i}) + c)$, we just pick it's negative gradient like gradient descent:\n",
    "\n",
    "$$c = -\\left[\\frac{\\partial{L(y_{i}, f(x_{i}))}}{\\partial f(x_{i})}\\right]_{f = f_{m-1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0.8]])\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"basic gbrt\"\"\"\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)\n",
    "gbrt.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=42)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"use staged_predict to find the best n_estimators\"\"\"\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"use warm_start to learn incrementally, stop while error does not improve for 5 iters\"\"\"\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=10000, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RandomForestClassifier(random_state=42)\n",
      "Training the ExtraTreesClassifier(random_state=42)\n",
      "Training the LinearSVC(max_iter=100, random_state=42, tol=20)\n",
      "Training the MLPClassifier(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = LinearSVC(max_iter=100, tol=20, random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)\n",
    "\n",
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "for estimator in estimators:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9692, 0.9715, 0.859, 0.9635]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_val, y_val) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier([\n",
    "    (\"random_forest_clf\", random_forest_clf),\n",
    "    (\"extra_trees_clf\", extra_trees_clf),\n",
    "    (\"svm_clf\", svm_clf),\n",
    "    (\"mlp_clf\", mlp_clf),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9697"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_soft_clf = VotingClassifier([\n",
    "    (\"random_forest_clf\", random_forest_clf),\n",
    "    (\"extra_trees_clf\", extra_trees_clf),\n",
    "    (\"mlp_clf\", mlp_clf),\n",
    "], voting=\"soft\")\n",
    "voting_soft_clf.fit(X_train, y_train)\n",
    "\n",
    "voting_soft_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9681"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)\n",
    "    \n",
    "y_pred = rnd_forest_blender.predict(X_test_predictions)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
