{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic linear regression with one variable:\n",
    "\n",
    "$$y=\\theta_{0} + \\theta_{1}x$$\n",
    "\n",
    "what if linear model could not nicely fit training examples?<br>\n",
    "we can naturally extend linear model to polynomial model, for example:\n",
    "\n",
    "$$y=\\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2} + \\theta_{3}x^{3}$$\n",
    "\n",
    "this method can be conclude as: map original attibutes x to some new set of quantities $\\phi(x)$ (called features), and use the same set of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### least mean squares with features\n",
    "let $\\phi : \\mathbb{R}^{d} \\to \\mathbb{R}^{p}$ be a feature map, then original batch gradient descent:\n",
    "\n",
    "$$\\theta := \\theta + \\alpha\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}x^{(i)})x^{(i)}$$\n",
    "\n",
    "using features:\n",
    "\n",
    "$$\\theta := \\theta + \\alpha\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}\\phi(x^{(i)}))\\phi(x^{(i)})$$\n",
    "\n",
    "the above becomes computationally expensive when $\\phi(x)$ is high dimensional.<br>\n",
    "but we can observe that, if at some point , $\\theta$ can be represented as:\n",
    "\n",
    "$$\\theta = \\sum_{i=1}^{n}\\beta_{i}\\phi(x^{(i)})$$\n",
    "\n",
    "then in the next round:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\theta &:= \\theta + \\alpha\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}\\phi(x^{(i)}))\\phi(x^{(i)}) \\\\\n",
    "&=\\sum_{i=1}^{n}\\beta_{i}\\phi(x^{(i)}) + \\alpha\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}\\phi(x^{(i)}))\\phi(x^{(i)}) \\\\\n",
    "&=\\sum_{i=1}^{n}(\\beta_{i} + \\alpha(y^{(i)} - \\theta^{T}\\phi(x^{(i)})))\\phi(x^{(i)})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\theta$ can be also represented as a linear representation of $\\phi(x^{(i)})$<br>\n",
    "we can then derive $\\beta$'s update rule:\n",
    "\n",
    "$$\\beta_{i} := \\beta_{i} + \\alpha(y^{(i)} - \\sum_{j=1}^{n}\\beta_{j}\\phi(x^{(j)})^{T}\\phi(x^{(i)}))$$\n",
    "\n",
    "we only need to compute $\\left \\langle \\phi(x^{(j)}), \\phi(x^{(i)}) \\right \\rangle = \\phi(x^{(j)})^{T}\\phi(x^{(i)}))$ to update parameters no matter how high the feature dimension p is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel\n",
    "\n",
    "we define the kernel corresponding to the feature map $\\phi$ as a function that satisfying:\n",
    "\n",
    "$$K(x, z) := \\left \\langle \\phi(x), \\phi(z) \\right \\rangle$$\n",
    "\n",
    "define the kernel matrix:\n",
    "\n",
    "$$K_{ij} = K(x^{(i)},x^{(j)})$$\n",
    "\n",
    "properties of kernel matrix:\n",
    "\n",
    "1. symmetric, since $\\phi(x)^{T}\\phi(z) = \\phi(z)^{T}\\phi(x)$.\n",
    "2. positive semidefinite:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "z^{T}Kz=&\\sum_{i}\\sum_{j}z_{i}K_{ij}z_{j}\\\\\n",
    "=&\\sum_{i}\\sum_{j}z_{i}\\phi(x^{(i)})^{T}\\phi(x^{(j)})z_{j}\\\\\n",
    "=&\\sum_{i}\\sum_{j}z_{i}\\sum_{k}\\phi_{k}(x^{(i)})\\phi_{k}(x^{(j)})z_{j}\\\\\n",
    "=&\\sum_{k}\\sum_{i}\\sum_{j}z_{i}\\phi_{k}(x^{(i)})\\phi_{k}(x^{(j)})z_{j}\\\\\n",
    "=&\\sum_{k}\\left(\\sum_{i}z_{i}\\phi_{k}(x^{(i)})\\right)^2\\\\\n",
    "\\ge&\\ 0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "in the other hand, we have sufficient conditions for valid kernels:\n",
    "\n",
    "(mercer): let $ K: \\mathbb{R}^{d} \\times \\mathbb{R}^{d} \\mapsto \\mathbb{R}$. then for K be a valid kernel, it is necessary and sufficient that for any $\\left \\{ x^{(1)},...,x^{(n)} \\right \\} $, the corresponding kernel matrix is symmetric positive semi-definite.\n",
    "\n",
    "proof t.b.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margins\n",
    "\n",
    "consider logistic regression, where the probability $p(y=1|x;\\theta)$ is modeled by $h_{\\theta}(x)=\\sigma(\\theta^{T}x)$. we predict 1 on an input x if and only if $\\theta^{T}x >= 0.5$, the larger $\\theta^{T}x$ is, the more confidence we are.<br>\n",
    "the distance from the hyperplane is important.\n",
    "\n",
    "functional margin:\n",
    "\n",
    "$$\\hat{\\gamma}^{(i)}=y^{(i)}(w^{T}x^{(i)} + b)$$\n",
    "\n",
    "geometric margin:\n",
    "\n",
    "$$\\gamma^{(i)}=\\frac{y^{(i)}(w^{T}x^{(i)} + b)}{\\left \\| w \\right \\| }$$\n",
    "\n",
    "geometric margin is the euclid distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the optimal margin classifier\n",
    "\n",
    "we want to maximize geometic margin:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\gamma, w, b}{max}\\ &\\gamma \\\\\n",
    "s.t\\quad &\\frac{y^{(i)}(w^{T}x^{(i)} + b)}{\\left \\| w \\right \\| } >= \\gamma\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "without loss of generality, we can set $\\gamma\\left \\| w \\right \\|=1$, then the above is equivalent to:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{w, b}{min}\\ &\\frac{1}{2}{\\left \\| w \\right \\|}^2 \\\\\n",
    "s.t\\quad &{y^{(i)}(w^{T}x^{(i)} + b)} >= 1\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lagrange duality\n",
    "\n",
    "consider the following primal optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{w}{min}\\quad &f(w)\\\\\n",
    "s.t\\quad &g_{i}(w) \\le 0,i=1,...,k\\\\\n",
    "&h_{i}(w)=0, i=1,...,l\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we define the lagrangian of this optimization problem:\n",
    "\n",
    "$$L(w,\\alpha,\\beta)=f(w) + \\sum_{i=1}^{k}\\alpha_{i}g_{i}(w) + \\sum_{i=1}^{l}\\beta_{i}h_{i}(w)$$\n",
    "\n",
    "here $\\alpha_{i}, \\beta_{i}$ are the lagrange multipliers. consider the quantity:\n",
    "\n",
    "$$\\theta_{P}(w) = \\underset{\\alpha,\\beta:\\alpha_{i}\\ge{0}}{max}L(w,\\alpha,\\beta) $$\n",
    "\n",
    "here $P$ stands for \"primal\".\n",
    "\n",
    "let some $w$ be given, if $w$ violates any primal constraints, i.e., if either $g_{i}(w) > 0$ or $h_{i}(w) \\ne 0$, then:\n",
    "\n",
    "$$\\theta_{P}(w) = \\underset{\\alpha,\\beta:\\alpha_{i}\\ge{0}}{max}f(w) + \\sum_{i=1}^{k}\\alpha_{i}g_{i}(w) + \\sum_{i=1}^{l}\\beta_{i}h_{i}(w) = \\infty$$\n",
    "\n",
    "conversely, if the constraints are saitistied for particular $w$, then $\\theta_{P}(w) = f(w)$, hence:\n",
    "\n",
    "$$\n",
    "\\theta_{P}(w) = \n",
    "\\begin{cases}\n",
    "f(w)\\ &\\text{if w satisfy constraints}\\\\\n",
    "\\infty\\ &\\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "with this equation, we have:\n",
    "\n",
    "$$\\underset{w}{min}f(w),\\ \\text{satisfy constraints} \\Leftrightarrow \\underset{w}{min}\\theta_{P}(w)=\\underset{w}{min}\\underset{\\alpha,\\beta:\\alpha_{i}\\ge{0}}{max}L(w,\\alpha,\\beta)$$\n",
    "\n",
    "we define the optimal value of the objective function to be $p^{\\ast} = \\underset{w}{min}\\theta_{P}(w)$.\n",
    "\n",
    "reverse the min max oder, we define:\n",
    "\n",
    "$$\\theta_{D}(\\alpha, \\beta) = \\underset{w}{min}L(w,\\alpha,\\beta)$$\n",
    "\n",
    "D stands for \"dual\", we can pose the dual optimization problem:\n",
    "\n",
    "$$\\underset{\\alpha,\\beta:\\alpha_{i}\\ge{0}}{max}\\theta_{D}(\\alpha, \\beta) = \\underset{\\alpha,\\beta:\\alpha_{i}\\ge{0}}{max}\\underset{w}{min}L(w,\\alpha,\\beta)$$\n",
    "\n",
    "we define the optimal $d^{\\ast}=\\underset{\\alpha,\\beta:\\alpha_{i}\\ge{0}}{max}\\theta_{D}(\\alpha, \\beta)$, it can easily be shown that:\n",
    "\n",
    "$$d^{\\ast}=\\underset{\\alpha,\\beta:\\alpha_{i}\\ge{0}}{max}\\theta_{D}(\\alpha, \\beta) \\le \\underset{w}{min}\\theta_{P}(w) = p^{\\ast}$$\n",
    "\n",
    "however, under certain conditions, we have:\n",
    "\n",
    "$$d^{\\ast} = p^{\\ast}$$\n",
    "\n",
    "so that we can solve the dual problem instead of the primal problem.\n",
    "\n",
    "(KKT conditions)suppose $f$ and $g_{i}$ are convex, $h_{i}$ are affine, and there exits some $w$ so that $g_{i}(w) < 0$ for all $i$. <br>\n",
    "under these assumptions, there must exists $w^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}$ so that $w^{\\ast}$ is the solution to the primal problem, $\\alpha^{\\ast},\\beta^{\\ast}$ are the solutions to the dual problem, and $p^{\\ast}=d^{\\ast}=L(w^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast})$.<br>\n",
    "moreover, $w^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}$ iff satisfy the KKT conditions:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial w_{i}}L(w^{\\ast},\\alpha^{\\ast},\\beta^{\\ast}) =& 0,\\ i=1,...,d\\\\\n",
    "\\frac{\\partial}{\\partial \\beta_{i}}L(w^{\\ast},\\alpha^{\\ast},\\beta^{\\ast}) =& 0,\\ i=1,...,l\\\\\n",
    "\\alpha_{i}^{\\ast}g_{i}(w) =& 0,\\ i=1,...,k\\\\\n",
    "g_{i}(w^{\\ast}) \\le& 0,\\ i=1,...,k\\\\\n",
    "\\alpha_{i}^{\\ast} \\ge& 0,\\ i=1,...,k\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "proof t.b.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 use lagrange duality\n",
    "\n",
    "previous optimal margin problem:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{w, b}{min}\\ &\\frac{1}{2}{\\left \\| w \\right \\|}^2 \\\\\n",
    "s.t\\quad &{y^{(i)}(w^{T}x^{(i)} + b)} >= 1\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we can write the constraints as:\n",
    "\n",
    "$$g_{i}(w) = 1 - y^{(i)}(w^{T}x^{(i)} + b) \\le 0$$\n",
    "\n",
    "this problem satisfy the KKT prerequisites:\n",
    "\n",
    "1. $\\frac{1}{2}{\\left \\| w \\right \\|}^2$ is convex.\n",
    "2. $1 - y^{(i)}(w^{T}x^{(i)} + b)$ is convex.\n",
    "3. there exits $w, b$, such that $1 - y^{(i)}(w^{T}x^{(i)} + b) < 0$, just increase the order.\n",
    "\n",
    "write the lagrangian of this problem:\n",
    "\n",
    "$$\n",
    "L(w,b,\\alpha )=\\frac{1}{2}\\left \\| w \\right \\|^{2} - \\sum_{i=1}^{n}\\alpha_{i}\\left [ y^{(i)}(w^{T}x^{(i)} + b) - 1 \\right ]     \n",
    "$$\n",
    "\n",
    "just solve the dual form of the problem, to do so, we need to first minimize $L(w,\\alpha,\\beta)$ with respect to $w,b$, setting derivatives to 0:\n",
    "\n",
    "$$\\nabla_{w}L(w, b, \\alpha) = w - \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}x^{(i)} = 0$$\n",
    "\n",
    "this implies:\n",
    "\n",
    "$$w = \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}x^{(i)}$$\n",
    "\n",
    "derivative with respect to b:\n",
    "\n",
    "$$\\nabla_{b}L(w, b, \\alpha) = \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0$$\n",
    "\n",
    "plug these equations back to lagrangian:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(w, b, \\alpha) =& \\frac{1}{2}\\left \\| w \\right \\|^{2} - \\sum_{i=1}^{n}\\alpha_{i}\\left [ y^{(i)}(w^{T}x^{(i)} + b) - 1 \\right ]\\\\\n",
    "=& \\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}x^{(i)})^{T}(\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}x^{(i)}) - \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}(\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}x^{(i)})^{T}x^{(i)} + b\\sum_{i=1}^{n}\\alpha_{i}y^{(i)} + \\sum_{i=1}^{n}\\alpha_{i}\\\\\n",
    "=& \\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}(x^{(i)})^{T}x^{(i)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we thus obtain the following dual optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\alpha}{max}\\quad &W(\\alpha)=\\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}\\left \\langle x^{(i)},x^{(j)} \\right \\rangle \\\\\n",
    "s.t\\quad &\\alpha_{i}\\ge{0},\\ i=1,...,n \\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "this is easier to solve(we'll talk about it later). after abtained $\\alpha_{i}^{\\ast}$, we have:\n",
    "\n",
    "$$w^{\\ast} = \\sum_{i=1}^{n}\\alpha_{i}^{\\ast}y^{(i)}x^{(i)}$$\n",
    "\n",
    "go back to the original problem, we get:\n",
    "\n",
    "$$b^{\\ast} = -\\frac{max_{i: y^{(i)}=-1}w^{\\ast{T}}x^{(i)} + min_{i: y^{(i)}=1}w^{\\ast{T}}x^{(i)}}{2} $$\n",
    "\n",
    "when making predictions, we have:\n",
    "\n",
    "$$w^{T}x + b = \\left ( \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}x^{(i)}\\right )^{T}x + b = \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}\\left \\langle x^{(i)},x \\right \\rangle + b$$\n",
    "\n",
    "we only needed inner product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-separable case\n",
    "\n",
    "so far, we assumed the data is linearly separable.\n",
    "\n",
    "while mapping data to a high dimensional feature space via $\\phi$ does not increase the likelihood that the data is separable.\n",
    "\n",
    "so we need to make the algorithm work for non-linearly separable datasets.\n",
    "\n",
    "we reformulate our optimization as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{w, b}{min}\\ &\\frac{1}{2}{\\left \\| w \\right \\|}^2 + C\\sum_{i=1}^{n}\\xi_{i}\\\\\n",
    "s.t\\quad &{y^{(i)}(w^{T}x^{(i)} + b)} >= 1 - \\xi_{i},\\ i=1,...,n\\\\\n",
    "&\\xi_{i} \\ge 0,\\ i=1,...,n.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "the cost of outlier $C\\xi_{i}$.\n",
    "\n",
    "the lagrangian:\n",
    "\n",
    "$$\n",
    "    L(w,b,\\xi,\\alpha,r )=\\frac{1}{2}\\left \\| w \\right \\|^{2} + C\\sum_{i=1}^{n}\\xi_{i} - \\sum_{i=1}^{n}\\alpha_{i}\\left [ y^{(i)}(w^{T}x^{(i)} + b) - 1 + \\xi_{i}\\right ] -\\sum_{i=1}^{n}r_{i}\\xi_{i}     \n",
    "$$\n",
    "\n",
    "here $\\alpha_{i},r_{i}$ are our lagrange multipliers.\n",
    "\n",
    "the dual form of the problem:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\alpha}{max}\\quad &W(\\alpha)=\\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}\\left \\langle x^{(i)},x^{(j)} \\right \\rangle \\\\\n",
    "s.t\\quad &0 \\le \\alpha_{i}\\le{C},\\ i=1,...,n \\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "the same as before except $\\alpha_{i}$'s constraints, the calculation for $b^{\\ast}$ has to be modified, t.b.c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, 2:]\n",
    "y = (iris[\"target\"] == 2).astype(np.int)\n",
    "\n",
    "svm_clf = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMO algorithm\n",
    "\n",
    "consider when trying to solve the unconstrained optimization problem:\n",
    "\n",
    "$$\\underset{\\alpha}{max}\\ W(\\alpha_{1},...,\\alpha_{n})$$\n",
    "\n",
    "we have the coordinate ascent algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\text{loop until convergence:}\\\\\n",
    "&\\qquad \\text{for }i=1,...,n:\\\\\n",
    "&\\qquad\\qquad \\alpha_{i}:=\\underset{\\hat{\\alpha_{i}}}{argmax}\n",
    "W(\\alpha_{1},...,\\alpha_{i-1},\\hat{\\alpha_{i}},\\alpha_{i+1},...,\\alpha_{n}).\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we can not directly use coordinate ascent facing SVM dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\alpha}{max}\\quad &W(\\alpha)=\\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}\\left \\langle x^{(i)},x^{(j)} \\right \\rangle \\\\\n",
    "s.t\\quad &0 \\le \\alpha_{i}\\le{C},\\ i=1,...,n \\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "if we want to update some $\\alpha_{i}$, we must update at least two of them simutaneously.\n",
    "\n",
    "coordinate ascent change to:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\text{loop until convergence:}\\\\\n",
    "&\\qquad 1.\\text{select some pair } \\alpha_{i}, \\alpha_{j}.\\\\\n",
    "&\\qquad 2.\\text{optimize } W(\\alpha) \\text{ with respect to } \\alpha_{i}, \\alpha_{j}, \\text{while holding other } \\alpha_{k} \\text{ fixed}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "the reason that SMO is an efficient is that the update to $\\alpha_{i}, \\alpha_{j}$ can be computed efficiently.\n",
    "\n",
    "take $i,j=1,2$ as example, SMO step is like:\n",
    "\n",
    "$$max\\ W(\\alpha_{1},\\alpha_{2},...,\\alpha_{n}) \\text{ while } $$\n",
    "$$\\alpha_{3},...,\\alpha_{n} \\text{ fixed}$$\n",
    "$$\\alpha_{1}y^{(1)} + \\alpha_{1}y^{(1)} = \\zeta$$\n",
    "$$0 \\le \\alpha_{1} \\le{C},0 \\le \\alpha_{2} \\le{C},$$\n",
    "\n",
    "this can be change to:\n",
    "\n",
    "$$max\\ W(\\alpha_{1},(\\zeta - \\alpha_{1}y^{(1)})y^{(2)},...,\\alpha_{n}) \\text{ while } L\\le \\alpha_{1}\\le H$$\n",
    "\n",
    "this is direct quadratic optimization, easy to solve.\n",
    "\n",
    "remained questions:\n",
    "\n",
    "1. the choice of $\\alpha_{i},\\alpha_{j}$ , this is heuristic.\n",
    "2. how to update b.\n",
    "\n",
    "t.b.c according to Platt's paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with kernels\n",
    "\n",
    "let $\\phi : \\mathbb{R}^{d} \\to \\mathbb{R}^{p}$ be a feature map, the original dual form of the problem:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\alpha}{max}\\quad &W(\\alpha)=\\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}\\left \\langle x^{(i)},x^{(j)} \\right \\rangle \\\\\n",
    "s.t\\quad &\\alpha_{i}\\ge{0},\\ i=1,...,n \\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "now change to:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\alpha}{max}\\quad &W(\\alpha)=\\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}\\left \\langle \\phi(x^{(i)}),\\phi(x^{(j)}) \\right \\rangle \\\\\n",
    "s.t\\quad &\\alpha_{i}\\ge{0},\\ i=1,...,n \\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we only need to know $\\left \\langle \\phi(x^{(i)}),\\phi(x^{(j)}) \\right \\rangle$ to optimize.\n",
    "\n",
    "when predicting:\n",
    "\n",
    "$$w^{T}\\phi(x) + b = \\left ( \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}\\phi(x^{(i)})\\right )^{T}x + b = \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}\\left \\langle \\phi(x^{(i)}),\\phi(x) \\right \\rangle + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge', random_state=42))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"PolynomialFeatures + LinearSVC\"\"\"\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"poly SVC\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"rbf SVC\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"SVM can support regression\"\"\"\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}