{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Classification\n",
    "\n",
    "regression answers how much? or how many? questions, for example:\n",
    "\n",
    "1. predict the number of dollors at which a house will be sold.\n",
    "2. predict the revenue of a restaurant.\n",
    "\n",
    "in practice, we are more often interested in classification: asking not \"how much\", but \"which one\":\n",
    "\n",
    "1. does this email belong in the spam foler or the inbox?\n",
    "2. does this image depict a donkey, a dog, a cat, or a rooster?\n",
    "3. which movie is Jean most likely to watch next?\n",
    "\n",
    "linear regression not fit to solve classification problem for two reasons:\n",
    "\n",
    "1. linear regression range in $\\mathbb{R}$, while classification label is discrete.\n",
    "2. linear regression uses euclid distance, result in d(class 3, class 1) > d(class 2, class 1), this is often not true in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "we first foucus on binary classification\n",
    "\n",
    "suppose dataset $D=\\left \\{ (x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)}) \\right \\} $, where $x^{(i)} \\in \\mathbb{R}^{d},\\ y^{(i)} \\in \\left \\{ 0, 1\\right \\}$.\n",
    "\n",
    "to tackle this problem, after deriving $\\theta^{T}x$\n",
    "\n",
    "we uses a function that transform value in $\\mathbb{R}$ into value in $[0, 1]$ and view this as the probability of being positive\n",
    "\n",
    "we choose:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "as that function, it is the so called sigmoid function, we choose sigmoid function because:\n",
    "\n",
    "1. it is a monotony increase, differentiable, symmetric function that maps $\\mathbb{R}$ into $[0,1]$.\n",
    "2. simple form\n",
    "3. derivative can calculate easily: ${\\sigma}'(x) = \\sigma(x)(1 - \\sigma(x))$\n",
    "\n",
    "now the model is:\n",
    "\n",
    "$$h_{\\theta}(x) = \\frac{1}{1 + exp(-\\theta^{T}x)}$$\n",
    "\n",
    "this is the logistic regression model.\n",
    "\n",
    "additionally, in our view, we have:\n",
    "\n",
    "\n",
    "$$h_{\\theta}(x) = p(y=1|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "self information $I(x)$ indicates the amount of information an event $x$ to happen.\n",
    "\n",
    "we want $I(x)$ to satisfy:\n",
    "\n",
    "1. $I(x) \\ge 0$\n",
    "2. $I(x) = 1 \\text{ if }p(x)=1$\n",
    "3. $\\text{if }p(x_{1}) > p(x_{2}) \\text{, then } I(x_{1}) < I(x_{2})$\n",
    "4. $I(x_{1}, x_{2}) = I(x_{1}) + I(x_{2}) \\text{ for independent }x_{1},x_{2}$\n",
    "\n",
    "this leads to $I(x) = -log\\ p(x)$\n",
    "\n",
    "while self-information measures the information of a single discrete event, entropy measures the information of a random variable:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "H(X)=&E(I(x))\\\\\n",
    "=&E(-log\\ p(x))\\\\\n",
    "=&-\\sum_{x \\in \\mathcal{X}}log\\ p(x)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "it is exactly the optimal encoding length of $X$.\n",
    "\n",
    "cross entropy $H(p, q)$ is the encoding length of $p$ by optimal encoding of $q$:\n",
    "\n",
    "$$H(p,q)=E_{p}\\left[-log\\ q(x)\\right] = -\\sum_{x}p(x)log\\ q(x)$$\n",
    "\n",
    "fix $p$, the closer $q$ is to $p$, the less is $H(p,q)$. \n",
    "\n",
    "we can use $H(p,q)$ to define the distance of $q$ to $p$.\n",
    "\n",
    "turn to our binary classification problem, for $y^{(i)} \\in \\left\\{0,1\\right\\}, \\hat{y}^{(i)} \\in (0, 1)$, we have:\n",
    "\n",
    "$$\n",
    "H(y^{(i)}, \\hat{y}^{(i)}) =\n",
    "\\begin{cases}\n",
    "-log(\\hat{y}^{(i)}),\\text{ if } y^{(i)} = 1\\\\\n",
    "-log(1 - \\hat{y}^{(i)}),\\text{ if } y^{(i)} = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "combine the two cases:\n",
    "\n",
    "$$H(y^{(i)}, \\hat{y}^{(i)}) = -y^{(i)}log(\\hat{y}^{(i)}) - (1-y^{(i)})log(1 - \\hat{y}^{(i)})$$\n",
    "\n",
    "we use cross entropy to define the loss of logistic regression model:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "J(\\theta) =& \\sum_{i=1}^{n}H(y^{(i)}, H(\\hat{y}^{(i)})) \\\\\n",
    "=& \\sum_{i=1}^{n}-y^{(i)}log(\\hat{y}^{(i)}) - (1-y^{(i)})log(1 - \\hat{y}^{(i)}) \\\\\n",
    "=& \\sum_{i=1}^{n}-y^{(i)}log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)})log(1 - h_{\\theta}(x^{(i)}))\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "this is the so called logistic regression.\n",
    "\n",
    "in addition, we can write cross entropy loss in matrix form:\n",
    "\n",
    "$$J(\\theta) = -y^{T}log(\\sigma(\\theta^{T}X)) - (1 - y)^{T}log(1 - \\sigma(\\theta^{T}X))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris[\"data\"][:, 2:]\n",
    "y = (iris[\"target\"] == 2).astype(np.int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Interpretation of Cross Entropy Loss\n",
    "\n",
    "as we suppose\n",
    "\n",
    "$$p(y=1|x) = h_{\\theta}(x)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$p(y=0|x) = 1 - h_{\\theta}(x)$$\n",
    "\n",
    "combine these two, in our prediction:\n",
    "\n",
    "$$p(y^{(i)}|x^{(i)}) = \\left [ h_{\\theta}(x^{(i)}) \\right ]^{y^{(i)}}\\left [ 1 - h_{\\theta}(x^{(i)}) \\right ]^{1 - y^{(i)}} $$\n",
    "\n",
    "log likelikhood function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(\\theta) &= log\\prod_{i=1}^{n} \\left [ h_{\\theta}(x^{(i)}) \\right ]^{y^{(i)}}\\left [ 1 - h_{\\theta}(x^{(i)}) \\right ]^{1 - y^{(i)}} \\\\\n",
    "&= \\sum_{i=1}^{n}\\left [y^{(i)}log\\ h_{\\theta}(x^{(i)}) + (1 - y^{(i)})log(1 - h_{\\theta}(x^{(i)})) \\right ]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "maximum the above likelihood is equal to minimize the cross entropy loss:\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^{n}-y^{(i)}log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)})log(1 - h_{\\theta}(x^{(i)}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update-rule\n",
    "\n",
    "we have:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial }{\\partial \\theta_{j}}J(\\theta ) &= \\frac{\\partial }{\\partial \\theta_{j}}\\sum_{i=1}^{n}-log(\\sigma(\\theta^{T}x^{(i)}))y^{(i)} - log(1 - \\sigma(\\theta^{T}x^{(i)}))(1 - y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{n} \\left (-y^{(i)}\\frac{1}{\\sigma(\\theta^{T}x^{(i)})} + (1 - y^{(i)})\\frac{1}{1 - \\sigma(\\theta^{T}x^{(i)})} \\right )\\frac{\\partial }{\\partial \\theta_{j}}\\sigma(\\theta^{T}x^{(i)})\\\\\n",
    "&=\\sum_{i=1}^{n} \\left (-y^{(i)}\\frac{1}{\\sigma(\\theta^{T}x^{(i)})} + (1 - y^{(i)})\\frac{1}{1 - \\sigma(\\theta^{T}x^{(i)})} \\right )\\sigma(\\theta^{T}x^{(i)})(1-\\sigma(\\theta^{T}x^{(i)}))\\frac{\\partial }{\\partial \\theta_{j}}\\theta^{T}x^{(i)} \\\\\n",
    "&=\\sum_{i=1}^{n}(\\sigma(\\theta^{T}x^{(i)}) - y^{(i)})x_{j}^{(i)} \\\\\n",
    "&=\\sum_{i=1}^{n}(h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "the same form with linear regression!\n",
    "\n",
    "as linear regression, we have the update rule for logistic regression:\n",
    "\n",
    "$$\\theta_{j}: =\\theta_{j} - \\alpha\\sum_{i=1}^{n} (h_{\\theta }(x^{(i)}) - y^{(i)})x_{j}^{(i)} $$\n",
    "\n",
    "combine all dimensions, we have:\n",
    "\n",
    "$$\\theta: =\\theta - \\alpha\\sum_{i=1}^{n} (h_{\\theta }(x^{(i)}) - y^{(i)})\\cdot x^{(i)} $$\n",
    "\n",
    "write in matrix form：\n",
    "$$\n",
    "\\frac{\\partial }{\\partial \\theta}J(\\theta ) = X^{T}(\\sigma(X\\theta) -y)\n",
    "$$\n",
    "matrix form of update formula：\n",
    "$$\n",
    "\\theta: =\\theta - \\alpha X^{T}(\\sigma(X\\theta)-\\mathbf{y} )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "like linear regression, we add penalty term on $J(\\theta)$ for regularization\n",
    "\n",
    "$l2$ penalty：\n",
    "$$J(\\theta) := J(\\theta) + \\lambda \\left \\| \\theta \\right \\|_{2}^{2} $$\n",
    "\n",
    "$l1$ penalty：\n",
    "$$J(\\theta) := J(\\theta) + \\lambda \\left \\| \\theta \\right \\|_{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "now we turn to multi-class classification.\n",
    "\n",
    "we start off with a simple image classification problem, each input consists of a $2\\times{2}$ grayscale image, represent each pixel with a scalar, giving us features $\\left\\{x_{1},x_{2},x_{3}, x_{4}\\right\\}$. assume each image belong to one among the categories \"cat\", \"chiken\" and \"dog\".\n",
    "\n",
    "we have a nice way to represent categorical data: the one-hot encoding, i.e a vector with as many components as we have categories, the component corresponding to particular instance's category is 1 and all others are 0.\n",
    "\n",
    "for our problem, \"cat\" represents by $(1,0,0)$, \"chicken\" by $(0, 1, 0)$, \"dog\" by $(0, 0, 1)$.\n",
    "\n",
    "to estimate the conditional probabilities of all classes, we need a model with multiple outputs, one per class.\n",
    "\n",
    "address classification with linear models, we will need as many affine functions as we have outputs:\n",
    "\n",
    "$$o_{1} = x_{1}w_{11} + x_{2}w_{12} + x_{3}w_{13} + x_{4}w_{14}$$\n",
    "$$o_{2} = x_{1}w_{21} + x_{2}w_{22} + x_{3}w_{23} + x_{4}w_{24}$$\n",
    "$$o_{3} = x_{1}w_{31} + x_{2}w_{32} + x_{3}w_{33} + x_{4}w_{34}$$\n",
    "\n",
    "depict as:\n",
    "\n",
    "![jupyter](./images/softmaxreg.svg)\n",
    "\n",
    "we would like output $\\hat{y_{j}}$ to be interpreted as probability that a given item belong to class $j$.\n",
    "\n",
    "to transform our current outputs $\\left\\{o_{1},o_{2},o_{3},o_{4}\\right\\}$ to probability distribution $\\left\\{\\hat{y}_{1},\\hat{y}_{2},\\hat{y}_{3},\\hat{y}_{4}\\right\\}$, we use the softmax operation:\n",
    "\n",
    "$$\\hat{y}_{j} = \\frac{exp(o_{j})}{\\sum_{k}exp(o_{k})}$$\n",
    "\n",
    "when predicting:\n",
    "\n",
    "$$\\text{predict class} = \\underset{j}{argmax}\\ \\hat{y}_{j} = \\underset{j}\\ o_{j}$$\n",
    "\n",
    "$\\hat{y}_{j}$ is necessary when compute loss.\n",
    "\n",
    "as logistic regression, we use the cross entropy loss:\n",
    "\n",
    "$$H(y,\\hat{y}) = -\\sum_{k}y_{j}log\\ \\hat{y}_{j} = -log\\ \\hat{y}_{\\text{category of y}}$$\n",
    "\n",
    "now complete the construction of softmax regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, multi_class='multinomial')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = iris[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = softmax_reg.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}