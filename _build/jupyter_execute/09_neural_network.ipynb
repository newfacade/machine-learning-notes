{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neuron\n",
    "\n",
    "a neuron takes input $x \\in \\mathbb{R}^{d}$, multiply $x$ by weights $w$ and add bias term $b$, finally use a activation function $g$.\n",
    "\n",
    "that is:\n",
    "\n",
    "$$f(x) = g(w^{T}x + b)$$\n",
    "\n",
    "it is analogous to the functionality of biological neuron.\n",
    "\n",
    "![jupyter](images/neuron.svg)\n",
    "\n",
    "some useful activation function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{sigmoid:}\\quad &g(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\text{tanh:}\\quad &g(z) = \\frac{e^{z}-e^{-z}}{e^{z} + e^{-z}} \\\\\n",
    "\\text{relu:}\\quad &g(z) = max(z,0) \\\\\n",
    "\\text{leaky relu:}\\quad &g(z) = max(z, \\epsilon{z})\\ ,\\ \\epsilon\\text{ is a small positive number}\\\\\n",
    "\\text{identity:}\\quad &g(z) = z\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "linear regression's forward process is a neuron with identity activation function.\n",
    "\n",
    "logistic regression's forward process is a neuron with sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network\n",
    "\n",
    "building neural network is analogous to lego bricks: you take individual bricks and stack them together to build complex structures.\n",
    "\n",
    "![jupyter](images/mlp.svg)\n",
    "\n",
    "we use bracket to denote layer, we take the above as example\n",
    "\n",
    "$[0]$ denote input layer, $[1]$ denote hidden layer, $[2]$ denote output layer\n",
    "\n",
    "$a^{[l]}$ denote the output of layer $l$, set $a^{[0]} := x$\n",
    "\n",
    "$z^{[l]}$ denote the affine result of layer $l$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$\n",
    "\n",
    "where $W^{[l]} \\in \\mathbb{R}^{d[l] \\times d[l-1]}$, $b^{[l]} \\in \\mathbb{R}^{d[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight decay\n",
    "\n",
    "recall that to mitigate overfitting, we use $l_{2}$ and $l_{1}$ regularization in linear and logistic regression.\n",
    "\n",
    "weight decay is a alias of $l_{2}$ regularization, can be generalize to neural network, we concatenate $W^{[l]}$ and flatten it to get $w$ in this setting.\n",
    "\n",
    "first adding $l_{2}$ norm penalty:\n",
    "\n",
    "$$J(w,b) = \\sum_{i=1}^{n}l(w, b, x^{(i)}, y^{(i)}) + \\frac{\\lambda}{2}\\left \\|  w \\right \\|^{2} $$\n",
    "\n",
    "then by gradient descent, we have:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "w:=& w-\\eta\\frac{\\partial}{\\partial w}J(w, b) \\\\\n",
    "=& w-\\eta\\frac{\\partial}{\\partial w}\\left(\\sum_{i=1}^{n}l(w, b, x^{(i)}, y^{(i)}) + \\frac{\\lambda}{2}\\left \\|  w \\right \\|^{2}\\right) \\\\\n",
    "=& (1 - \\eta\\lambda)w - \\eta\\frac{\\partial}{\\partial w}\\sum_{i=1}^{n}l(w, b, x^{(i)}, y^{(i)})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "multiply by $(1 - \\eta\\lambda)$ is weight decay.\n",
    "\n",
    "often we do not calculate bias term in regularization, so does weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropout\n",
    "\n",
    "to strength robustness through perturbation, we can deliberately add perturbation in traning, dropout is one of that skill.\n",
    "\n",
    "we actually do the following in hidden neuron:\n",
    "\n",
    "$$\n",
    "a_{dropout} = \n",
    "\\begin{cases}\n",
    "0 &\\text{with probability }p \\\\\n",
    "\\frac{a}{1-p} &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "this operation randomly dropout neuron with probability $p$ and keep the expectation unchanged:\n",
    "\n",
    "$$E(a_{dropout}) = E(a)$$\n",
    "\n",
    "depict this process below:\n",
    "\n",
    "![jupyter](images/dropout2.svg)\n",
    "\n",
    "one more thing: we do not use dropout in predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prerequesities for back-propagation\n",
    "\n",
    "suppose in forward-propagation $x \\to y \\to l$, where $x \\in \\mathbb{R}^{n}$, $y \\in \\mathbb{R} ^{m}$, loss $l \\in \\mathbb{R}$.\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial y} = \\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial y_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\frac{\\partial l}{\\partial x} = \\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial x_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "by total differential equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial x_{k}} = \\sum_{j=1}^{m}\\frac{\\partial l}{\\partial y_{j}}\\frac{\\partial y_{j}}{\\partial x_{k}}\n",
    "$$\n",
    "\n",
    "then we can connect $\\frac{\\partial l}{\\partial x}$ and $\\frac{\\partial l}{\\partial y}$ by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial x} = \\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial x_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & ... & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    " \\vdots  & \\ddots  & \\vdots \\\\\n",
    "  \\frac{\\partial y_{1}}{\\partial x_{n}}& .... & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial y_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "(\\frac{\\partial y}{\\partial x})^{T}\\frac{\\partial l}{\\partial y}\n",
    "$$\n",
    "\n",
    "here $\\frac{\\partial y}{\\partial x}$ is the jacobian matrix.\n",
    "\n",
    "unlike other activation functions, calculate softmax depend on other neurons, so jacobian of softmax.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_{i}}{\\partial z_{j}} = \\frac{\\partial}{\\partial z_{j}}\\left(\\frac{exp(z_{i})}{\\sum_{s=1}^{k}exp(z_{s})}\\right)\n",
    "$$\n",
    "\n",
    "it is easy to check the jacobian of matrix-multiplication:\n",
    "\n",
    "$$\\frac{\\partial Mx}{\\partial x} = M$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## back-propagation\n",
    "\n",
    "gradient descent update rule:\n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha\\frac{\\partial{L}}{\\partial{W^{[l]}}}$$\n",
    "\n",
    "$$b^{[l]} = b^{[l]} - \\alpha\\frac{\\partial{L}}{\\partial{b^{[l]}}}$$\n",
    "\n",
    "to proceed, we must compute the gradient with respect to the parameters.\n",
    "\n",
    "we can define a three-step recipe for computing the gradients as follows:\n",
    "\n",
    "1.for output layer, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(\\hat{y}, y)}{\\partial z^{[N]}} = (\\frac{\\partial \\hat{y}}{\\partial z^{[N]}})^{T}\\frac{\\partial L(\\hat{y}, y)}{\\partial \\hat{y}}\n",
    "$$\n",
    "\n",
    "if $g^{[N]}$ is softmax.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(\\hat{y}, y)}{\\partial z^{[N]}} = \\frac{\\partial L(\\hat{y}, y)}{\\partial \\hat{y}} \\odot {g^{[N]}}'(z^{[N]})\n",
    "$$\n",
    "\n",
    "if not softmax.\n",
    "\n",
    "the above computations are all straight forward.\n",
    "\n",
    "2.for $l=N-1,...,1$, we have:\n",
    "\n",
    "$$z^{[l + 1]} = W^{[l + 1]}a^{[l]} + b^{[l + 1]}$$\n",
    "\n",
    "so by our prerequesities:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a^{[l]}} = (\\frac{\\partial z^{[l+1]}}{\\partial a^{[l]}})^{T}\\frac{\\partial L}{\\partial z^{[l+1]}} = (W^{[l+1]})^{T}\\frac{\\partial L}{\\partial z^{[l+1]}}\n",
    "$$\n",
    "\n",
    "we also have:\n",
    "\n",
    "$$a^{[l]} = g^{[l]}z^{[l]}$$\n",
    "\n",
    "we do not use softmax activation in hidden layers, so the dependent is direct:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^{[l]}} = \\frac{\\partial L}{\\partial a^{[l]}} \\odot {g^{[l]}}'(z^{[l]})$$\n",
    "\n",
    "combine two equations:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^{[l]}} = (W^{[l+1]})^{T}\\frac{\\partial L}{\\partial z^{[l+1]}} \\odot {g^{[l]}}'(z^{[l]})$$\n",
    "\n",
    "3.final step, because:\n",
    "\n",
    "$$z^{[l]} = W^{[l]}a^{[l - 1]} + b^{[l]}$$\n",
    "\n",
    "so:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial z^{[l]}}(a^{[l - 1]})^{T}$$ \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^{[l]}}=\\frac{\\partial L}{\\partial z^{[l]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xavier initialization\n",
    "\n",
    "to mitigate vanishing and exploding gradient, to insure breaking symmtry, we should carefully initialize weights.\n",
    "\n",
    "consider a fully connected layer without bias term and activation function:\n",
    "\n",
    "$$o_{i} = \\sum_{j=1}^{n_{in}}w_{ij}x_{j}$$\n",
    "\n",
    "suppose $w_{ij}$ draw from a distribution of 0 mean and $\\sigma^{2}$ variance, not necessarily guassian.\n",
    "\n",
    "suppose $x_{j}$ draw from a distribution of 0 mean and $\\gamma^{2}$ variance, all $w_{ij}, x_{j}$ are independent.\n",
    "\n",
    "then mean of $o_{i}$ is of course 0, variance:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Var[o_{i}] =& E[o_{i}^{2}] - (E[o_{i}])^{2}\\\\\n",
    "=&\\sum_{j=1}^{n_{in}}E[w_{ij}^{2}x_{j}^{2}] \\\\\n",
    "=&\\sum_{j=1}^{n_{in}}E[w_{ij}^{2}]E[x_{j}^{2}] \\\\\n",
    "=&n_{in}\\sigma^{2}\\gamma^{2}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "to keep variance fixed, we need to set $n_{in}\\sigma^{2}=1$.\n",
    "\n",
    "consider back-propagation, we have:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_{j}} = \\sum_{i=1}^{n_{out}}w_{ij}\\frac{\\partial L}{\\partial o_{i}}$$\n",
    "\n",
    "so by the same inference, we need to set $$n_{out}\\sigma^{2} = 1$$.\n",
    "\n",
    "we cannot satisfy both conditions simutaneously, we simply try to satisfy:\n",
    "\n",
    "$$\\frac{1}{2}(n_{in} + n_{out})\\sigma^{2} = 1 \\ \\text{ or }\\ \\sigma = \\sqrt{\\frac{n_{in} + n_{out}}{2}}$$\n",
    "\n",
    "this is the reasoning under xavier initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}