{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative & Generative\n",
    "\n",
    "discriminative：try to learn $p(y|x)$ directly, such as logistic regression<br>\n",
    "or try to learn mappings from the space of inputs to the labels $\\left \\{ 0, 1\\right \\}$ directly, such as perceptron\n",
    "\n",
    "generative：algorithms that try to model $p(x|y)$ and $p(y)$(called class prior), such as guassian discriminant analysis and naive bayes\n",
    "\n",
    "when predicting, use bayes rule：\n",
    "\n",
    "$$p(y|x)=\\frac{p(x|y)p(y)}{p(x)}$$\n",
    "\n",
    "then：\n",
    "\n",
    "$$\\hat{y}=\\underset{y}{argmax}\\ \\frac{p(x|y)p(y)}{p(x)}= \\underset{y}{argmax}\\ {p(x|y)p(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guassian Discriminant Analysis(GDA)\n",
    "\n",
    "multivariant normal distribution\n",
    "\n",
    "guassian distribution is parameterized by a mean vector $\\mu \\in \\mathbb{R}^{d}$ and a covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$, where $\\Sigma >= 0$ is symmetric and positive semi-definite. also written $\\mathcal{N}(\\mu,\\Sigma)$, it's density is given by：\n",
    "\n",
    "$$p(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{d/2}\\left | \\Sigma \\right |^{1/2} }exp\\left ( -\\frac{1}{2}(x-\\mu)^{T}{\\Sigma}^{-1}(x-\\mu)\\right )$$\n",
    "\n",
    "unsurprisingly, for random variable $X \\sim \\mathcal{N}(\\mu,\\Sigma)$:\n",
    "\n",
    "$$E[X] = \\int_{x}xp(x;\\mu, \\Sigma)dx = \\mu$$\n",
    "$$Cov(X) = E[(X - E(X))(X - E(X))^{T}] = \\Sigma$$\n",
    "\n",
    "GDA:\n",
    "\n",
    "when we have a classification problem in which the input features x are continous, we can use GDA, which model $p(x|y)$ using a multivariant normal distribution:\n",
    "\n",
    "$$y\\sim Bernoulli(\\phi)$$\n",
    "$$x | y=0 \\sim \\mathcal{N}(\\mu_{0},\\Sigma) $$\n",
    "$$x | y=1 \\sim \\mathcal{N}(\\mu_{1},\\Sigma) $$\n",
    "\n",
    "writing out the distribution:\n",
    "\n",
    "$$p(y) = \\phi^{y}(1 - \\phi)^{1 - y}$$\n",
    "\n",
    "$$p(x| y=0)=\\frac{1}{(2\\pi)^{d/2}\\left | \\Sigma \\right |^{1/2} }exp\\left (-\\frac{1}{2}(x-\\mu_{0})^{T}{\\Sigma}^{-1}(x-\\mu_{0})\\right )$$\n",
    "\n",
    "$$p(x| y=1)=\\frac{1}{(2\\pi)^{d/2}\\left | \\Sigma \\right |^{1/2} }exp\\left (-\\frac{1}{2}(x-\\mu_{1})^{T}{\\Sigma}^{-1}(x-\\mu_{1})\\right )$$\n",
    "\n",
    "the log-likelihood of the data is given by：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\phi,\\mu_{0},\\mu_{1},\\Sigma) &= log\\prod_{i=1}^{n}p(x^{(i)},y^{(i)};\\phi,\\mu_{0},\\mu_{1},\\Sigma) \\\\\n",
    "&= log\\prod_{i=1}^{n}p(x^{(i)}|y^{(i)};\\mu_{0},\\mu_{1},\\Sigma)p(y^{(i)};\\phi)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we find the maximum likelihood estimate of the parameters are：\n",
    "\n",
    "$$\\phi = \\frac{1}{n}\\sum_{i=1}^{n}1\\left \\{ y^{(i)}=1 \\right \\}$$\n",
    "\n",
    "$$\\mu_{0} = \\frac{\\sum_{i=1}^{n}1\\left \\{ y^{(i)}=0 \\right \\}x^{(i)}  }{\\sum_{i=1}^{n}1\\left \\{ y^{(i)}=0 \\right \\}} $$\n",
    "\n",
    "$$\\mu_{1} = \\frac{\\sum_{i=1}^{n}1\\left \\{ y^{(i)}=1 \\right \\}x^{(i)}  }{\\sum_{i=1}^{n}1\\left \\{ y^{(i)}=1 \\right \\}} $$\n",
    "\n",
    "$$\\Sigma=\\frac{1}{n}\\sum_{i=1}^{n}(x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}$$\n",
    "\n",
    "as we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDA and logistic regression\n",
    "\n",
    "the GDA model has an interesting relationship to logistic regression.\n",
    "\n",
    "if we view the quantity $p(y=1|x; \\phi,\\Sigma,\\mu_{0},\\mu_{1})$ as a function of $x$, we'll find that it can be expressed in the form:\n",
    "\n",
    "$$\n",
    "p(y=1|x; \\phi,\\Sigma,\\mu_{0},\\mu_{1}) = \\frac{1}{1 + exp(-\\theta^{T}x)}\n",
    "$$\n",
    "\n",
    "where $\\theta$ is some appropriate function of $\\phi,\\Sigma,\\mu_{0},\\mu_{1}$.<br>\n",
    "proof:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(y=1|x; \\phi,\\Sigma,\\mu_{0},\\mu_{1}) &= \\frac{p(y=1, x)}{p(x)} \\\\\n",
    "&=\\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1) + p(x|y=1)p(y=1)} \\\\\n",
    "&=\\frac{\\phi\\cdot exp\\left (-\\frac{1}{2}(x-\\mu_{1})^{T}{\\Sigma}^{-1}(x-\\mu_{1})\\right )}{\\phi\\cdot exp\\left (-\\frac{1}{2}(x-\\mu_{1})^{T}{\\Sigma}^{-1}(x-\\mu_{1})\\right ) + (1 - \\phi)\\cdot exp\\left (-\\frac{1}{2}(x-\\mu_{0})^{T}{\\Sigma}^{-1}(x-\\mu_{0})\\right )} \\\\\n",
    "&=\\frac{1}{1 + \\frac{1 - \\phi}{\\phi}exp\\left(-\\frac{1}{2}\\left((x-\\mu_{0})^{T}{\\Sigma}^{-1}(x-\\mu_{0}) - (x-\\mu_{1})^{T}{\\Sigma}^{-1}(x-\\mu_{1})\\right)\\right)} \\\\\n",
    "&=\\frac{1}{1 + \\frac{1 - \\phi}{\\phi}exp\\left(-\\frac{1}{2}\\left((\\mu_{1}^T -\\mu_{0}^T)\\Sigma^{-1}x + x^{T}\\Sigma^{-1}(\\mu_{1}-\\mu_{0}) + (\\mu_{0}^{T}\\Sigma^{-1}\\mu_{0} - \\mu_{1}^{T}\\Sigma^{-1}\\mu_{1})\\right)\\right)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "because of $x^{T}a=a^{T}x$, we can express the above as:\n",
    "\n",
    "$$\\frac{1}{1 + exp(-\\theta^{T}x)}$$\n",
    "\n",
    "so the separating surface of GDA is $\\theta^{T}x=0$.\n",
    "\n",
    "as logistic regression, GDA can be interpreted by logistic model, but with different optimization strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "in GDA，x is continous, when x is discrete, we can use naive bayes.\n",
    "\n",
    "suppose $x=(x_{1}, x_{2},..., x_{d})$, for simplicity, we assume $x_{j}$ binary here, we of course have：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(x|y) &= p(x_{1},x_{2},...,x_{d}|y) \\\\\n",
    "&= p(x_{1}|y)p(x_{2}|y,x_{1})...p(x_{d}|y,x_{1},...,x_{d-1})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "we will assume that $x_{j}$'s are conditionally independent given $y$. this assumption is called the naive bayes assumption, and the resulting algorithm is called the naive bayes classifier：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(x|y) &= p(x_{1}|y)p(x_{2}|y,x_{1})...p(x_{d}|y,x_{1},...,x_{d-1}) \\\\\n",
    "&= p(x_{1}|y)p(x_{2}|y)...p(x_{d}|y)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "our model is parameterized by:\n",
    "\n",
    "$$y\\sim Bernoulli(\\phi)$$\n",
    "$$x_{j}|y=1 \\sim Bernoulli(\\phi_{j|y=1})$$\n",
    "$$x_{j}|y=0 \\sim Bernoulli(\\phi_{j|y=0})$$\n",
    "\n",
    "note:\n",
    "\n",
    "$$k = \\sum_{i=1}^{n}1\\left\\{y^{(i)}=1\\right\\}$$\n",
    "$$s_{j} = \\sum_{i=1}^{n}1\\left\\{x_{j}^{(i)}=1 \\wedge y^{(i)}=1\\right\\}$$\n",
    "$$t_{j} = \\sum_{i=1}^{n}1\\left\\{x_{j}^{(i)}=1 \\wedge y^{(i)}=0\\right\\}$$\n",
    "\n",
    "we can write down the joint log likelihood of the data:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathcal{L}(\\phi, \\phi_{j|y=1}, \\phi_{j|y=0}) &= log\\prod_{i=1}^{n}p(x^{(i)},y^{(i)})\\\\\n",
    "&=\\sum_{i=1}^{n}log(p(x^{(i)}, y^{(i)})) \\\\\n",
    "&=\\sum_{i=1}^{n}log(p(y^{(i)})\\prod_{j=1}^{d}p(x_{j}^{(i)}|y^{(i)})) \\\\\n",
    "&=\\sum_{y^{(i)}=1}(log(\\phi) + \\sum_{j=1}^{d}log(p(x_{j}^{(i)}|y=1))) + \\sum_{y^{(i)}=0}(log(1 - \\phi) + \\sum_{j=1}^{d}log(p(x_{j}^{(i)}|y=0))) \\\\\n",
    "&=k\\ log(\\phi) + (n-k)log(1 - \\phi) + \\sum_{j=1}^{d}(s_{j}\\ log(\\phi_{j|y=1}) + (k-s_{j})log(1 - \\phi_{j|y=1}) + t_{j}\\ log(\\phi_{j|y=0}) + (n -k - t_{j})log(1 - \\phi_{j|y=0})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "maximizing this equal to maximize each part, we derive:\n",
    "\n",
    "$$\\phi=\\frac{k}{n}$$\n",
    "$$\\phi_{j|y=1} = \\frac{s_{j}}{k}$$\n",
    "$$\\phi_{j|y=0} = \\frac{t_{j}}{n-k}$$\n",
    "\n",
    "as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing\n",
    "\n",
    "there is problem with the naive bayes in its current form\n",
    "\n",
    "if $x_{j}=1$ never occur in the training set, then $p(x_{j}=1|y=1)=0,p(x_{j}=1|y=0)=0$.\n",
    "\n",
    "hence when predicting a sample $x$ with $x_{j}=1$, then:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(y=1|x) &= \\frac{\\prod_{k=1}^{d}p(x_{k}|y=1)p(y=1)}{\\prod_{k=1}^{d}p(x_{k}|y=1)p(y=1) + \\prod_{k=1}^{d}p(x_{k}|y=0)p(y=0)} \\\\\n",
    "&= \\frac{0}{0}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "does't know how to predict.\n",
    "\n",
    "to fix this problem, instead of:\n",
    "\n",
    "$$\\phi_{j|y=1}=\\frac{\\sum_{i=1}^{n}1\\left\\{x_{j}^{(i)}=1 \\wedge y^{(i)}=1\\right\\}}{\\sum_{i=1}^{n}1\\left\\{y^{(i)}=1\\right\\}}$$\n",
    "\n",
    "$$\\phi_{j|y=0}=\\frac{\\sum_{i=1}^{n}1\\left\\{x_{j}^{(i)}=1 \\wedge y^{(i)}=0\\right\\}}{\\sum_{i=1}^{n}1\\left\\{y^{(i)}=0\\right\\}}$$\n",
    "\n",
    "we add 1 to the numerator, add 2 to the denominator to:\n",
    "\n",
    "1. avoid 0 in the parameter\n",
    "2. $\\phi_{j|y=1}$ and $\\phi_{j|y=0}$ is still a probability distribution.\n",
    "\n",
    "i.e:\n",
    "\n",
    "$$\\phi_{j|y=1}=\\frac{1 + \\sum_{i=1}^{n}1\\left\\{x_{j}^{(i)}=1 \\wedge y^{(i)}=1\\right\\}}{2 + \\sum_{i=1}^{n}1\\left\\{y^{(i)}=1\\right\\}}$$\n",
    "\n",
    "$$\\phi_{j|y=0}=\\frac{1 + \\sum_{i=1}^{n}1\\left\\{x_{j}^{(i)}=1 \\wedge y^{(i)}=0\\right\\}}{2 + \\sum_{i=1}^{n}1\\left\\{y^{(i)}=0\\right\\}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "bernoulli event model:<br>\n",
    "first randomly determined whether a spammer or non-spammer<br>\n",
    "then runs through the dictionary deciding whether to include each word j.\n",
    "\n",
    "multinomial event model:<br>\n",
    "first randomly determined whether a spammer or non-spammer<br>\n",
    "then each word in the email is generating from some same multinomial distribution independently.\n",
    "\n",
    "using multinomial event model, if we have training set $\\left \\{ (x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)}) \\right \\}$ where $x^{(i)}=(x_{1}^{(i)},...,x_{d_{i}}^{(i)})\\ $(here $d_{i}$ is the number of words in the i-th training example)\n",
    "\n",
    "using maximum likelihood estimates of parameters like the above:\n",
    "\n",
    "$$\\phi = \\frac{1}{n}\\sum_{i=1}^{n}1\\left \\{ y^{(i)}=1 \\right \\}$$\n",
    "\n",
    "$$\\phi_{k|y=1}=\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{d_{i}}1\\left \\{x_{j}^{(i)}=k\\wedge y^{(i)}=1 \\right \\}}{\\sum_{i=1}^{n}1\\left \\{ y^{(i)}=1 \\right \\}d_{i}}$$\n",
    "\n",
    "$$\\phi_{k|y=0}=\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{d_{i}}1\\left \\{x_{j}^{(i)}=k\\wedge y^{(i)}=0 \\right \\}}{\\sum_{i=1}^{n}1\\left \\{ y^{(i)}=0 \\right \\}d_{i}}$$\n",
    "\n",
    "add laplace smoothing with respect to multinomial event model:\n",
    "\n",
    "$$\\phi_{k|y=1}=\\frac{1 + \\sum_{i=1}^{n}\\sum_{j=1}^{d_{i}}1\\left \\{x_{j}^{(i)}=k\\wedge y^{(i)}=1 \\right \\}}{\\left | V \\right | + \\sum_{i=1}^{n}1\\left \\{ y^{(i)}=1 \\right \\}d_{i}}$$\n",
    "\n",
    "$$\\phi_{k|y=0}=\\frac{1 + \\sum_{i=1}^{n}\\sum_{j=1}^{d_{i}}1\\left \\{x_{j}^{(i)}=k\\wedge y^{(i)}=0 \\right \\}}{\\left | V \\right | + \\sum_{i=1}^{n}1\\left \\{ y^{(i)}=0 \\right \\}d_{i}}$$\n",
    "\n",
    "$\\left | V \\right |$ is the size of the vocabulary\n",
    "\n",
    "in short:\n",
    "\n",
    "$$\\phi_{k|y=1}=\\frac{1 + number\\ of\\ words\\ k\\ occur\\ in\\ spammer}{\\left | V \\right | + number\\ of\\ words\\ in\\ spammer}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}